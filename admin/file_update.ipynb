{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-identity in /opt/conda/lib/python3.11/site-packages (1.20.0)\n",
      "Requirement already satisfied: azure-core>=1.31.0 in /opt/conda/lib/python3.11/site-packages (from azure-identity) (1.32.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in /opt/conda/lib/python3.11/site-packages (from azure-identity) (41.0.4)\n",
      "Requirement already satisfied: msal>=1.30.0 in /opt/conda/lib/python3.11/site-packages (from azure-identity) (1.31.1)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from azure-identity) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from azure-identity) (4.8.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from azure-core>=1.31.0->azure-identity) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from azure-core>=1.31.0->azure-identity) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.11/site-packages (from cryptography>=2.5->azure-identity) (1.16.0)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity) (2.8.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from msal-extensions>=1.2.0->azure-identity) (2.10.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tick\n",
      "FMC query at page_index=1\n",
      "FMC query at page_index=2\n",
      "FMC query at page_index=3\n",
      "FMC query at page_index=4\n",
      "FMC query at page_index=5\n",
      "FMC query at page_index=6\n",
      "FMC query at page_index=7\n",
      "FMC query at page_index=8\n",
      "FMC query at page_index=9\n",
      "FMC query at page_index=10\n",
      "Found 9655 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9655/9655 [00:11<00:00, 843.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting add of 7781 tasks.\n",
      "tick\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AccessToken' object has no attribute 'expiry'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 431\u001b[0m\n\u001b[1;32m    427\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m60\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m(\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[12], line 426\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtick\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 426\u001b[0m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m60\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 369\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m():\n\u001b[1;32m    368\u001b[0m     organization_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnrcs-2-pf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 369\u001b[0m     fmc_token \u001b[38;5;241m=\u001b[39m \u001b[43mrequest_fmc_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43morganization_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;66;03m# fmc_query = 'Car.licensePlate = \"LBXQ6155\" and Sequence.recordingDate > \"2025-01-01\" and ReferenceFile.type = \"PCAP\" and ReferenceFile.type = \"JSON_METADATA\"'\u001b[39;00m\n\u001b[1;32m    371\u001b[0m     fmc_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCar.licensePlate = \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLBXQ6155\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and Sequence.recordingDate > \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2025-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and ReferenceFile.type = \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCAP\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and ReferenceFile.type = \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJSON_METADATA\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and MeasurementFile.path ~ \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1P_DE_LBXQ6155_ZEUS\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and MeasurementFile.contentType ~ \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytesoup\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[12], line 64\u001b[0m, in \u001b[0;36mrequest_fmc_token\u001b[0;34m(organization_name, stage)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest_fmc_token\u001b[39m(organization_name, stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprod\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    Get fmc token via AzureCliCredential. (Requires az login beforehand).\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    organization_name e.g. nrcs-2-pf, ford-dat-3, uss-gen-6-pf\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcachedCredential\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapi://api-data-loop-platform-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43morganization_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/.default\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoken\n",
      "Cell \u001b[0;32mIn[12], line 42\u001b[0m, in \u001b[0;36mCachedCredential.get_token\u001b[0;34m(self, scope, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, scope: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AccessToken:\n\u001b[1;32m     41\u001b[0m   token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token\u001b[38;5;241m.\u001b[39mget(scope)\n\u001b[0;32m---> 42\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpiry\u001b[49m \u001b[38;5;241m<\u001b[39m time\u001b[38;5;241m.\u001b[39mtime():\n\u001b[1;32m     43\u001b[0m     calc_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token[scope] \u001b[38;5;241m=\u001b[39m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelegate\u001b[38;5;241m.\u001b[39mget_token(scope, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AccessToken' object has no attribute 'expiry'"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from azure.identity import AzureCliCredential\n",
    "from azure.core.credentials import TokenCredential, AccessToken\n",
    "from requests import get\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import subprocess\n",
    "import re\n",
    "from typing import Any\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "from dataclasses import dataclass\n",
    "import dataclasses\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def calc_time(string):\n",
    "    global start_time\n",
    "\n",
    "    if string == \"start\":\n",
    "        start_time = time.time()\n",
    "    else:\n",
    "        end_time = time.time()\n",
    "        return  end_time - start_time\n",
    "\n",
    "\n",
    "class CachedCredential(TokenCredential):\n",
    "  def __init__(self, delegate: TokenCredential, logger) -> None:\n",
    "    self.delegate = delegate\n",
    "    self.logger = logger\n",
    "    self._token : dict[str, AccessToken] = {}\n",
    "\n",
    "  def get_token(self, scope: str, **kwargs) -> AccessToken:\n",
    "    token = self._token.get(scope)\n",
    "    if not token or token.expiry < time.time():\n",
    "      calc_time(\"start\")\n",
    "      self._token[scope] = token = self.delegate.get_token(scope, **kwargs)\n",
    "      elapsed_time = calc_time(\"end\")\n",
    "      self.logger.info(\n",
    "            f\"Time taken to generate token(CachedCredential) is {elapsed_time:.2f} seconds.\"\n",
    "        )\n",
    "    else:\n",
    "        self.logger.info(\n",
    "            f\"Valid token exists\"\n",
    "        )\n",
    "    return token\n",
    "\n",
    "\n",
    "cachedCredential = CachedCredential(AzureCliCredential(), logger)\n",
    "\n",
    "def request_fmc_token(organization_name, stage='prod'):\n",
    "    \"\"\"\n",
    "    Get fmc token via AzureCliCredential. (Requires az login beforehand).\n",
    "\n",
    "    organization_name e.g. nrcs-2-pf, ford-dat-3, uss-gen-6-pf\n",
    "    \"\"\"\n",
    "    return cachedCredential.get_token(f'api://api-data-loop-platform-{organization_name}-{stage}/.default').token\n",
    "\n",
    "\n",
    "def get_sequence(sequence_id, organization_name, fmc_token):\n",
    "    \"\"\"\n",
    "    Does get sequence Rest call for sequence_id. Returns sequence.\n",
    "    \"\"\"\n",
    "    fmc_headers = {\n",
    "        'Cache-Control': 'no-cache',\n",
    "        'Authorization': f'Bearer {fmc_token}',\n",
    "        'Origin': 'https://developer.bosch-data-loop.com'\n",
    "    }\n",
    "    url = f'https://api.azr.bosch-data-loop.com/measurement-data-processing/v3/organizations/{organization_name}/sequence/{sequence_id}'\n",
    "    response = get(url, headers=fmc_headers)\n",
    "    if response.status_code == 200:\n",
    "        sequence = response.json()\n",
    "        return sequence\n",
    "    else:\n",
    "        logger.error(f'Get sequence call to FMC failed. status_code: {response.status_code}, reason: {response.reason}, url: {url}')\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_sequences(fmc_query, organization_name, fmc_token):\n",
    "    \"\"\"\n",
    "    Does get sequences Rest call for fmc query. Returns list of sequences.\n",
    "    \"\"\"\n",
    "    fmc_headers = {\n",
    "        'Cache-Control': 'no-cache',\n",
    "        'Authorization': f'Bearer {fmc_token}',\n",
    "        'Origin': 'https://developer.bosch-data-loop.com'\n",
    "    }\n",
    "\n",
    "    sequences = []\n",
    "\n",
    "    items_per_page = 1000\n",
    "\n",
    "    is_there_more_sequences = True\n",
    "    page_index = 0\n",
    "    while is_there_more_sequences:\n",
    "        url = f'https://api.azr.bosch-data-loop.com/measurement-data-processing/v3/organizations/{organization_name}/sequence?itemsPerPage={items_per_page}&pageIndex={page_index}&filterQuery={fmc_query}'  # noqa: E501\n",
    "        response = get(url, headers=fmc_headers)\n",
    "        if response.status_code == 200:\n",
    "            response_sequences = response.json()\n",
    "            sequences.extend(response_sequences)\n",
    "            if len(response_sequences) < items_per_page:\n",
    "                is_there_more_sequences = False\n",
    "        else:\n",
    "            logger.error(f'Get sequences call to FMC failed. status_code: {response.status_code}, reason: {response.reason}, url: {url}')\n",
    "            is_there_more_sequences = False\n",
    "        page_index += 1\n",
    "        print(f'FMC query at {page_index=}')\n",
    "\n",
    "    return sequences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: for testing, should be false later\n",
    "SUPPORT_MF4 = True\n",
    "DO_TESTING = os.getenv('DO_TESTING', '0') != '0'\n",
    "\n",
    "# TODO: update url\n",
    "if DO_TESTING:\n",
    "    REST_API_URL = 'http://localhost:7100'\n",
    "else:\n",
    "    REST_API_URL = 'http://fe-c-017ev.lr.de.bosch.com:7100'\n",
    "\n",
    "REST_API_HEADERS = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Sequence:\n",
    "    _id: str\n",
    "    fmc_data: Any\n",
    "    checksum: str | None = None\n",
    "    sia_meas_id_path: str | None = None\n",
    "    referenceFileTypes: list[str] | None = None\n",
    "    isManuallyLabeled: bool | None = None\n",
    "    label_date_epoch: int | None = None\n",
    "    label_labeler: str | None = None\n",
    "    label_bolf_path: str | None = None\n",
    "    assigned_labeler: str | None = None\n",
    "\n",
    "\n",
    "def set_sequence_measurement_checksums(sequences: list[Sequence]):\n",
    "    for sequence in sequences:\n",
    "\n",
    "        checksum = None\n",
    "        for meas_file in sequence.fmc_data['measurementFiles']:\n",
    "            if 'bytesoup' in meas_file['path']:\n",
    "                checksum = meas_file['checksum']\n",
    "            if SUPPORT_MF4 and meas_file['path'].endswith('.mf4'):\n",
    "                checksum = meas_file['checksum']\n",
    "\n",
    "        if checksum is not None:\n",
    "            sequence.checksum = checksum\n",
    "\n",
    "\n",
    "def set_referenceFileTypes(sequences: list[Sequence]):\n",
    "    for sequence in sequences:\n",
    "        sequence.referenceFileTypes = [i['type'] for i in sequence.fmc_data['referenceFiles']]\n",
    "\n",
    "\n",
    "def set_labeled(sequences: list[Sequence]):\n",
    "    siaqua_path = '/home/jovyan/data/ReadOnly/dypersiaqua/nrcs-2-pf/'\n",
    "    siadev_path = '/home/jovyan/data/ReadOnly/dypersiadev/nrcs-2-pf/'\n",
    "\n",
    "    siaqua_blobstore_url = 'https://dypersiaqua.blob.core.windows.net/nrcs-2-pf/'\n",
    "    siadev_blobstore_url = 'https://dypersiadev.blob.core.windows.net/nrcs-2-pf/'\n",
    "\n",
    "    label_paths = []\n",
    "    for dir, url in [(siadev_path, siadev_blobstore_url), (siaqua_path, siaqua_blobstore_url)]:\n",
    "        output = subprocess.check_output(['find', '.', '-type', 'f'], cwd=dir).splitlines()\n",
    "        for line in output:\n",
    "            line = line.decode().strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith('./'):\n",
    "                line = line[2:]\n",
    "            label_paths.append((line, url))\n",
    "\n",
    "    for sequence in sequences:\n",
    "        if not sequence.checksum:\n",
    "            continue\n",
    "\n",
    "        # '06120852e9ace6ce4285dc8943c0ea362c7b843cc7bb0efa4251fc778a8fa014/processed_lidar/2025_01_14_17_06_55/06120852e9ace6ce4285dc8943c0ea362c7b843cc7bb0efa4251fc778a8fa014_ebd7rng_2025_01_14_17_06_55.json'\n",
    "        pattern = re.compile(f'^{sequence.checksum}/processed_lidar/(?P<date>[^/]+)/{sequence.checksum}_(?P<labeler>[^_]+).+.json$')\n",
    "\n",
    "        for label_path, sia_url in label_paths:\n",
    "            match = re.match(pattern, label_path)\n",
    "            if not match:\n",
    "                continue\n",
    "\n",
    "            date_epoch = int(time.mktime(datetime.datetime.strptime(match.group('date'), '%Y_%m_%d_%H_%M_%S').timetuple()))\n",
    "            if not sequence.label_date_epoch or date_epoch > sequence.label_date_epoch:\n",
    "                sequence.label_date_epoch = date_epoch\n",
    "                sequence.label_labeler = match.group('labeler')\n",
    "                sequence.label_bolf_path = os.path.join(sia_url, label_path)\n",
    "            sequence.isManuallyLabeled = True\n",
    "\n",
    "\n",
    "def set_sia_link(sequences: list[Sequence]):\n",
    "    pattern = re.compile(r'https://(?P<blob_store>[^\\.]+).blob.core.windows.net(?P<path>.*)')\n",
    "\n",
    "    for sequence in sequences:\n",
    "        for referenceFile in sequence.fmc_data['referenceFiles']:\n",
    "            if referenceFile['type'] != 'ROF':\n",
    "                continue\n",
    "            rof_path = referenceFile['path']\n",
    "            match = re.match(pattern, rof_path)\n",
    "            if not match:\n",
    "                print('Warning: unknown rof path', rof_path)\n",
    "                continue\n",
    "\n",
    "            path_query = quote(match.group('path'), safe='')\n",
    "            sequence.sia_meas_id_path = f'%2F{match.group(\"blob_store\")}{path_query}'\n",
    "\n",
    "\n",
    "def get_sequence_id_to_bolf_path(sequences: list[Sequence], organization_name):\n",
    "    ret = {}\n",
    "    for sequence in sequences:\n",
    "        if sequence.label_bolf_path:\n",
    "            ret[sequence._id] = { \n",
    "                'path': sequence.label_bolf_path,\n",
    "                'type_of_label': 'KPI_GENERATED_BOLF'\n",
    "            }\n",
    "\n",
    "    return {\n",
    "        organization_name: ret\n",
    "    }\n",
    "\n",
    "\n",
    "def get_labeled_sequences(sequences: list[Sequence]):\n",
    "    ret = []\n",
    "    for sequence in sequences:\n",
    "        if sequence.label_bolf_path:\n",
    "            ret.append({\n",
    "                'measurement_checksum': sequence.checksum,\n",
    "                'label_bolf_path': sequence.label_bolf_path\n",
    "            })\n",
    "    \n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_labeler_ranking(sequences: list[Sequence]):\n",
    "    labelers: dict[str, int] = defaultdict(int)\n",
    "\n",
    "    for sequence in sequences:\n",
    "        if sequence.label_labeler:\n",
    "            labelers[sequence.label_labeler] += 1\n",
    "\n",
    "    return labelers\n",
    "\n",
    "\n",
    "# copied from main.py\n",
    "@dataclass\n",
    "class LabelTaskCreate:\n",
    "    fmc_id: str\n",
    "    fmc_data: str\n",
    "    measurement_checksum: str\n",
    "    sia_meas_id_path: str\n",
    "\n",
    "\n",
    "def send_new_tasks(sequences: list[Sequence]):\n",
    "    # sequence to labeltaskcreate list\n",
    "    tasks = []\n",
    "    for sequence in sequences:\n",
    "        # check if valid\n",
    "        if sequence.checksum and sequence.sia_meas_id_path:\n",
    "            tasks.append(dataclasses.asdict(\n",
    "                LabelTaskCreate(fmc_id=str(sequence._id), fmc_data=json.dumps(sequence.fmc_data), measurement_checksum=sequence.checksum, sia_meas_id_path=sequence.sia_meas_id_path)\n",
    "            ))\n",
    "    \n",
    "    print(f'Requesting add of {len(tasks)} tasks.')\n",
    "\n",
    "    with open('req_add_tasks.json', 'w') as f:\n",
    "        f.write(json.dumps(tasks))\n",
    "    return\n",
    "\n",
    "    r = requests.post(f'{REST_API_URL}/api/add_tasks', headers=REST_API_HEADERS, data=json.dumps(tasks))\n",
    "    if r.status_code == 200:\n",
    "        print('send new tasks successful')\n",
    "        return\n",
    "    \n",
    "    print('send new tasks failed', r.status_code)\n",
    "    print(r.text)\n",
    "\n",
    "\n",
    "def send_set_labeled(labeled_tasks: list[Sequence]):\n",
    "    with open('req_set_labeled.json', 'w') as f:\n",
    "        f.write(json.dumps(labeled_tasks))\n",
    "    return\n",
    "    \n",
    "    r = requests.post(f'{REST_API_URL}/api/set_labeled', headers=REST_API_HEADERS, data=json.dumps(labeled_tasks))\n",
    "    if r.status_code == 200:\n",
    "        print('send set labeled successful')\n",
    "        return\n",
    "    \n",
    "    print('send set labeled failed', r.status_code)\n",
    "    print(r.text)\n",
    "\n",
    "\n",
    "def fmcTimeToEpoch(ts: str) -> int:\n",
    "    return int(datetime.datetime.fromisoformat(ts.replace('Z', '+00:00')).timestamp())\n",
    "\n",
    "\n",
    "def check_video_exists(container, checksum, meas_name, d):\n",
    "    # sometimes video has bytesoup lz4 in name\n",
    "    cut_meas_name = meas_name.split('.')[0]\n",
    "    for name in list(set([cut_meas_name, meas_name])):\n",
    "        if os.path.exists(f'{container}/{checksum}/video_output/{name}_{d}.mp4'):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_fmc(sequence: Sequence):\n",
    "    fmc = sequence.fmc_data\n",
    "    container = '/home/jovyan/data/ReadOnly/dyperexprod/nrcs-2-pf'\n",
    "    # checksum = '3dbe19d68c1bc7c4d0bdc297062f4577ad60fa2920ba4d7842aef5cdf9c58893'\n",
    "    realWorldCutoffEpch = fmcTimeToEpoch('2025-02-08T01:01:01.000Z')\n",
    "\n",
    "    fmc_id = fmc['id']\n",
    "    meas_name = next((mf['path'].split('/')[-1] for mf in fmc['measurementFiles'] if 'bytesoup' in mf['path']), None)\n",
    "    checksum = next((mf['checksum'] for mf in fmc['measurementFiles'] if 'bytesoup' in mf['path']), None)\n",
    "    # if not checksum or not meas_name.startswith('1P_DE_LBXQ6155_ZEUS'): return [], [], []\n",
    "\n",
    "    add_task, missing_processedlidar, missing_frontvideo, missing_previewvideo, missing_rawpreviewvideo = [], [], [], [], []\n",
    "\n",
    "    has_lidar = False\n",
    "    if (\n",
    "            not os.path.exists(f'{container}/{checksum}/processed_lidar') or\n",
    "            len(os.listdir(f'{container}/{checksum}/processed_lidar')) < 2\n",
    "    ):\n",
    "        missing_processedlidar.append(fmc_id)\n",
    "    else:\n",
    "        has_lidar = True\n",
    "    \n",
    "    has_video = False\n",
    "    if fmcTimeToEpoch(fmc['recordingDate']) <= realWorldCutoffEpch:\n",
    "        if check_video_exists(container, checksum, meas_name, 'front'):\n",
    "            has_video = True\n",
    "        else:\n",
    "            missing_frontvideo.append(fmc_id)\n",
    "    else:\n",
    "        if check_video_exists(container, checksum, meas_name, 'preview'):\n",
    "            has_video = True\n",
    "        else:\n",
    "            raw_preview_available = any(referenceFile['type'] == 'PREVIEW_VIDEO_MERGED' for referenceFile in fmc['referenceFiles'])\n",
    "            if raw_preview_available:\n",
    "                missing_previewvideo.append(fmc_id)\n",
    "                has_video = True\n",
    "            else:\n",
    "                missing_rawpreviewvideo.append(fmc_id)\n",
    "\n",
    "    if has_lidar and has_video:\n",
    "        add_task.append(sequence)\n",
    "\n",
    "    return add_task, missing_processedlidar, missing_frontvideo, missing_previewvideo, missing_rawpreviewvideo\n",
    "\n",
    "\n",
    "def run():\n",
    "    organization_name = 'nrcs-2-pf'\n",
    "    fmc_token = request_fmc_token(organization_name)\n",
    "    # fmc_query = 'Car.licensePlate = \"LBXQ6155\" and Sequence.recordingDate > \"2025-01-01\" and ReferenceFile.type = \"PCAP\" and ReferenceFile.type = \"JSON_METADATA\"'\n",
    "    fmc_query = 'Car.licensePlate = \"LBXQ6155\" and Sequence.recordingDate > \"2025-01-01\" and ReferenceFile.type = \"PCAP\" and ReferenceFile.type = \"JSON_METADATA\" and MeasurementFile.path ~ \"1P_DE_LBXQ6155_ZEUS\" and MeasurementFile.contentType ~ \"bytesoup\"'\n",
    "    fmc_sequences = get_sequences(fmc_query, organization_name, fmc_token)\n",
    "\n",
    "    print(f'Found {len(fmc_sequences)} sequences.')\n",
    "\n",
    "    sequences = [Sequence(seq['id'], seq) for seq in fmc_sequences]\n",
    "    set_sequence_measurement_checksums(sequences)\n",
    "    set_referenceFileTypes(sequences)\n",
    "    set_sia_link(sequences)\n",
    "    set_labeled(sequences)\n",
    "\n",
    "    labeled_tasks = get_labeled_sequences(sequences)\n",
    "\n",
    "    add_task = []\n",
    "    missing_processedlidar = []\n",
    "    missing_frontvideo = []\n",
    "    missing_previewvideo = []\n",
    "    missing_rawpreviewvideo = []\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(check_fmc, sequences), total=len(fmc_sequences)))\n",
    "        for at, mpl, mfv, mpv, mrpv in results:\n",
    "            add_task.extend(at)\n",
    "            missing_processedlidar.extend(mpl)\n",
    "            missing_frontvideo.extend(mfv)\n",
    "            missing_previewvideo.extend(mpv)\n",
    "            missing_rawpreviewvideo.extend(mrpv)\n",
    "\n",
    "    with open('valid_ids.txt', 'w') as f:\n",
    "        f.write(json.dumps([str(s._id) for s in add_task]))\n",
    "    \n",
    "    with open('missing_data.json', 'w') as f:\n",
    "        f.write(json.dumps({\n",
    "            'missing_processedlidar': missing_processedlidar,\n",
    "            'missing_frontvideo': missing_frontvideo,\n",
    "            'missing_previewvideo': missing_previewvideo,\n",
    "            'missing_rawpreviewvideo': missing_rawpreviewvideo\n",
    "        }))\n",
    "\n",
    "    with open('valid_ids.txt', 'w') as f:\n",
    "        f.write(json.dumps([str(s._id) for s in add_task]))\n",
    "    \n",
    "    send_new_tasks(add_task)\n",
    " \n",
    "    send_set_labeled(labeled_tasks)\n",
    "\n",
    "    sequence_id_to_bolf_path = get_sequence_id_to_bolf_path(sequences, organization_name)\n",
    "    with open('labeltaskforce_bolfs_latest.json', 'w') as f:\n",
    "        f.write(json.dumps(sequence_id_to_bolf_path))\n",
    "        # TODO: automate to send to fmc\n",
    "\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        print('tick')\n",
    "        run()\n",
    "        time.sleep(60 * 5)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    raise SystemExit(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
